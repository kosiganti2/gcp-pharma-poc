# GCP Pharma Data Engineering PoC

## ğŸ¯ Overview
Advanced data engineering pipeline demonstrating:
- Multi-layered data processing (Bronze â†’ Silver â†’ Gold)
- Agentic AI integration for intelligent data enrichment
- Event-driven architecture with real-time processing
- Infrastructure as Code with Terraform
- Advanced analytics and ML workflows

## ğŸ—ï¸ Architecture
```
Raw Data (Bronze) â†’ Standardized (Silver) â†’ Analytics (Gold)
    â†“                      â†“                    â†“
  GCS Storage         BigQuery Tables      Analytics Views
    â†“                      â†“                    â†“
  PySpark ETL        AI Enrichment       Business Intelligence
```

## ğŸ“ Project Structure
```
gcp-pharma-poc/
â”œâ”€â”€ terraform/          # Infrastructure as Code
â”œâ”€â”€ pyspark_jobs/       # Data processing jobs
â”œâ”€â”€ dags/              # Airflow orchestration
â”œâ”€â”€ sql/               # Database schemas and queries
â”œâ”€â”€ schemas/           # Data schemas (Bronze/Silver/Gold)
â”œâ”€â”€ input/             # Sample data files
â”œâ”€â”€ docker/            # Container configurations
â””â”€â”€ docs/              # Documentation
```

## ğŸš€ Quick Start
1. Set up environment: `source .env`
2. Deploy infrastructure: `cd terraform && terraform apply`
3. Upload sample data: `gsutil cp input/* gs://bucket-name/raw/`
4. Trigger pipeline: Access Composer UI

## ğŸ“Š Key Features
- **Schema Evolution**: Automatic schema inference and evolution
- **Data Quality**: Comprehensive validation and monitoring
- **AI Enrichment**: ML-based data categorization and insights
- **Real-time Processing**: Event-driven data ingestion
- **Monitoring**: Complete observability with alerting

## ğŸ› ï¸ Technologies
- **Infrastructure**: Terraform, GCP
- **Processing**: PySpark, Dataproc
- **Orchestration**: Cloud Composer (Airflow)
- **Storage**: BigQuery, Cloud Storage
- **AI/ML**: Vertex AI, Custom UDFs
- **Monitoring**: Cloud Monitoring, Logging
